#!/usr/bin/env snakemake -s
##
## Snakefile to process rock/roi data
##
## Started 11th Oct 2023
##
## Izaskun Mallona
## GPLv3

# import os
import os.path as op
# from glob import glob
# import pandas as pd
# import re

configfile: "config.yaml"

print(config)
            
if config['simulate']:
    config['gtf'] = op.join(config['working_dir'], 'data', 'genome.gtf')
    config['genome'] = op.join(config['working_dir'], 'data', 'genome.fa')
    config['samples'] = [{'name': 'simulated',
                          'uses': {
                              'cdna_fq': op.join(config['working_dir'], 'data', 'simulated', 'r1.fq.gz'),
                              'cb_umi_fq': op.join(config['working_dir'], 'data', 'simulated', 'r2.fq.gz'),
                              'whitelist': '96x3',
                              'expected_cells': 500}},
                         {'name': 'simulated2', 'uses': {
                             'cdna_fq': op.join(config['working_dir'], 'data', 'simulated', 'r1.fq.gz'),
                             'cb_umi_fq': op.join(config['working_dir'], 'data', 'simulated', 'r2.fq.gz'),
                             'whitelist': '96x3', 'expected_cells': 500}}]
    config['capture_gtf_column_2_pattern'] = 'captured'
    config['run_mode'] = 'all'
    
def get_sample_names():
    return([x['name'] for x in config['samples']])

def get_cbumi_by_name(name):
    for i in range(len(config['samples'])):
        if config['samples'][i]['name'] == name:
             return(config['samples'][i]['uses']['cb_umi_fq'])

def get_cdna_by_name(name):
    for i in range(len(config['samples'])):
        if config['samples'][i]['name'] == name:
             return(config['samples'][i]['uses']['cdna_fq'])

def get_expected_cells_by_name(name):
    for i in range(len(config['samples'])):
        if config['samples'][i]['name'] == name:
             return(config['samples'][i]['uses']['expected_cells'])

def get_barcode_whitelist_by_name(name):
    for i in range(len(config['samples'])):
        if config['samples'][i]['name'] == name:
             return(config['samples'][i]['uses']['whitelist'])

## canonical CB 9-mers or just A{9}
## (used for simulations)
cb1s = ['GTCGCTATA','CTTGTACTA','CTTCACATA','ACACGCCGG','CGGTCCAGG','AATCGAATG','CCTAGTATA']
cb2s = ['TACAGGATA','CACCAGGTA','TGTGAAGAA','GATTCATCA','CACCCAAAG','CACAAAGGC','GTGTGTCGA']
cb3s = ['AAGCCTTCT','ATCATTCTG','CACAAGTAT','ACACCTTAG','GAACGACAA','AGTCTGTAC','AAATTACAG', 'AAAAAAAAA']
umis = ['AACCTTGG', 'CCGGTTAA', 'TTGGCCAA', 'GACATAGG']

rule all:
    input:
        expand(op.join(config['working_dir'], 'multimodal', '{sample}', '{sample}_sce.rds'),
               sample = get_sample_names())
    
rule index:
    input:
        gtf = config['gtf'],
        fa = config['genome']
    output:
        index_path =  op.join(config['working_dir'] , 'data', 'index', 'SAindex')
    threads:
        config['nthreads']
    params:
        simulate = config['simulate'],
        processing_path = op.join(config['working_dir'], 'data'),
        nthreads = config['nthreads'],
        star = config['STAR'],
        sjdbOverhang = config['sjdbOverhang']
    log:
        op.join(config['working_dir'], 'data', 'indexing.log')
    shell:
      """
    mkdir -p {params.processing_path}
    # cd {params.processing_path}

    ({params.star} --runThreadN {params.nthreads} \
     --runMode genomeGenerate \
     --sjdbGTFfile {input.gtf} \
     --genomeDir {params.processing_path}/index \
     --genomeSAindexNbases 4 \
     --sjdbOverhang {params.sjdbOverhang} \
     --genomeFastaFiles {input.fa} ) 2> {log}
        """

rule simulate_genome:
    output:
        op.join(config['working_dir'], 'data', 'genome.fa')
    shell:
        """
cat << EOF > {output}
>offtarget ERCC-00002
TCCAGATTACTTCCATTTCCGCCCAAGCTGCTCACAGTATACGGGCGTCG
GCATCCAGACCGTCGGCTGATCGTGGTTTTACTAGGCTAGACTAGCGTAC
GAGCACTATGGTCAGTAATTCCTGGAGGAATAGGTACCAAGAAAAAAACG
AACCTTTGGGTTCCAGAGCTGTACGGTCGCACTGAACTCGGATAGGTCTC
AGAAAAACGAAATATAGGCTTACGGTAGGTCCGAATGGCACAAAGCTTGT
TCCGTTAGCTGGCATAAGATTCCATGCCTAGATGTGATACACGTTTCTGG
AAACTGCCTCGTCATGCGACTGTTCCCCGGGGTCAGGGCCGCTGGTATTT
GCTGTAAAGAGGGGCGTTGAGTCCGTCCGACTTCACTGCCCCCTTTCAGC
CTTTTGGGTCCTGTATCCCAATTCTCAGAGGTCCCGCCGTACGCTGAGGA
CCACCTGAAACGGGCATCGTCGCTCTTCGTTGTTCGTCGACTTCTAGTGT
GGAGACGAATTGCCAGAATTATTAACTGCGCAGTTAGGGCAGCGTCTGAG
GAAGTTTGCTGCGGTTTCGCCTTGACCGCGGGAAGGAGACATAACGATAG
CGACTCTGTCTCAGGGGATCTGCATATGTTTGCAGCATACTTTAGGTGGG
CCTTGGCTTCCTTCCGCAGTCAAAACCGCGCAATTATCCCCGTCCTGATT
TACTGGACTCGCAACGTGGGTCCATCAGTTGTCCGTATACCAAGACGTCT
AAGGGCGGTGTACACCCTTTTGAGCAATGATTGCACAACCTGCGATCACC
TTATACAGAATTATCAATCAAGCTCCCCGAGGAGCGGACTTGTAAGGACC
GCCGCTTTCGCTCGGGTCTGCGGGTTATAGCTTTTCAGTCTCGACGGGCT
AGCACACATCTGGTTGACTAGGCGCATAGTCGCCATTCACAGATTTGCTC
GGCAATCAGTACTGGTAGGCGTTAGACCCCGTGACTCGTGGCTGAACGGC
CGTACAACTCGACAGCCGGTGCTTGCGTTTTACCCTTAAAAAAAAAAAAA
AAAAAAAAAAA
>ontarget ERCC-00003 dimer
CAGCAGCGATTAAGGCAGAGGCGTTTGTATCTGCCATTATAAAGAAGTTT
CCTCCAGCAACTCCTTTCTTAATTCCAAACTTAGCTTCAGTTATAAATTC
CCCTCCCATGATTGGGATTTTATAAACTTTTCTTCCATATAATTCATCTT
TCTTCTCATAACCGTCTCCGAAAAACTTCAACTTAAATCCAACCTTTAAC
TGCTCATCAGCCATGTCTCCCACAGCATCAAAAATAGCAGTTGTTGGACA
TGTTAAGACACACTGCCCCAATCTCTCTAACATTTGATGCTCTAACTCTG
ACTTTTTAGGGTGGCATATCTGTATTATAAATCCTGGTCTTCCATCTGGT
GTTTTTGATGGAGGGACATATTTCTCAATTCCTGCTTCTGCTGGACACAT
TATAACTGAACAACCAAAACCTGTTGCCTCTGTAGCTGCAATCTTAGCCC
ACTTCTTTGTAGCTGCTGTTATTAAAACTCTTGAAACCCATATTGGGAAT
GCTTCTGCAAATGTATCTTCAATATATACTCCATTTATTTCCATAGTTTC
CCTCCATTAAGATTTTAACAATTATAGTTTATCTTAGGGGCTATTAATAT
CTTATCATTTGGTTTTTAATATTCGATAAATCCATAAATAAAAATATATC
AACAATAATTTTAAATAATCTAAGTATAGGTAATATAACAATTAAAAAGA
TTTAGAGGGATAGAATTGAACGGCATTAGGAGAATTGTTTTAGATATATT
GAAGCCGCATGAGCCAAAAATAACAGATATGGCATTAAAATTAACATCAT
TATCAAACATTGATGGGGTTAATATTACAGTCTATGAAATAGATAAAGAG
ACTGAGAATGTTAAAGTTACAATTGAAGGGAATAATTTAGATTTTGATGA
GATTCAGGAAATTATTGAAAGTTTGGGAGGGACTATTCACAGTATAGATG
AGGTTGTTGCAGGTAAAAAGATTATTGAAGAGTTAGAACACCACAAGATA
AAAAAAAAAAAAAAAAAAAAAAACAGCAGCGATTAAGGCAGAGGCGTTTGTATCTGCCATTATAAAGAAGTTT
CCTCCAGCAACTCCTTTCTTAATTCCAAACTTAGCTTCAGTTATAAATTC
CCCTCCCATGATTGGGATTTTATAAACTTTTCTTCCATATAATTCATCTT
TCTTCTCATAACCGTCTCCGAAAAACTTCAACTTAAATCCAACCTTTAAC
TGCTCATCAGCCATGTCTCCCACAGCATCAAAAATAGCAGTTGTTGGACA
TGTTAAGACACACTGCCCCAATCTCTCTAACATTTGATGCTCTAACTCTG
ACTTTTTAGGGTGGCATATCTGTATTATAAATCCTGGTCTTCCATCTGGT
GTTTTTGATGGAGGGACATATTTCTCAATTCCTGCTTCTGCTGGACACAT
TATAACTGAACAACCAAAACCTGTTGCCTCTGTAGCTGCAATCTTAGCCC
ACTTCTTTGTAGCTGCTGTTATTAAAACTCTTGAAACCCATATTGGGAAT
GCTTCTGCAAATGTATCTTCAATATATACTCCATTTATTTCCATAGTTTC
CCTCCATTAAGATTTTAACAATTATAGTTTATCTTAGGGGCTATTAATAT
CTTATCATTTGGTTTTTAATATTCGATAAATCCATAAATAAAAATATATC
AACAATAATTTTAAATAATCTAAGTATAGGTAATATAACAATTAAAAAGA
TTTAGAGGGATAGAATTGAACGGCATTAGGAGAATTGTTTTAGATATATT
GAAGCCGCATGAGCCAAAAATAACAGATATGGCATTAAAATTAACATCAT
TATCAAACATTGATGGGGTTAATATTACAGTCTATGAAATAGATAAAGAG
ACTGAGAATGTTAAAGTTACAATTGAAGGGAATAATTTAGATTTTGATGA
GATTCAGGAAATTATTGAAAGTTTGGGAGGGACTATTCACAGTATAGATG
AGGTTGTTGCAGGTAAAAAGATTATTGAAGAGTTAGAACACCACAAGATA
AAAAAAAAAAAAAAAAAAAAAAA
EOF
        """

## notice the `captured` flag to focus on the last three GTF records during the custom featurecounting
rule simulate_gtf:
    output:
        op.join(config['working_dir'], 'data', 'genome.gtf')
    shell:
        """
echo -e 'offtarget\tERCC\texon\t1\t1061\t.\t+\t.\tgene_id "offtarget"; transcript_id "offtarget_1";
ontarget\tcaptured\texon\t1\t1023\t.\t+\t.\tgene_id "ontarget_1"; transcript_id "ontarget_1";
ontarget\tcaptured\texon\t100\t800\t.\t+\t.\tgene_id "ontarget_1b"; transcript_id "ontarget_1b";
ontarget\tcaptured\texon\t1030\t1090\t.\t+\t.\tgene_id "ontarget_2"; transcript_id "ontarget_2";' > {output}
        """

rule simulate_fastqs:
    input:
        part_r1 = expand(op.join(config['working_dir'], 'data', '{sample}',
                                 'part_{cb1}_{cb2}_{cb3}_{umi}_r1.fq.gz'),
                         sample = get_sample_names(),
                         cb1 = cb1s, cb2 = cb2s, cb3 = cb3s, umi = umis),
        part_r2 = expand(op.join(config['working_dir'], 'data', '{sample}',
                                 'part_{cb1}_{cb2}_{cb3}_{umi}_r2.fq.gz'),
                         sample = get_sample_names(),
                         cb1 = cb1s, cb2 = cb2s, cb3 = cb3s, umi = umis)
    params:
        path = op.join(config['working_dir'], 'data', '{sample}')
    output:
        r1 = op.join(config['working_dir'], 'data', '{sample}', 'r1.fq.gz'),
        r2 = op.join(config['working_dir'], 'data', '{sample}', 'r2.fq.gz'),
        r1_extra = temp(op.join(config['working_dir'], 'data', '{sample}', 'r1_extra.fq.gz')),
        r2_extra = temp(op.join(config['working_dir'], 'data', '{sample}', 'r2_extra.fq.gz')),

    shell:
        """
# cd {params.path}

seq 1 100 | awk ' {{print "@extra"$0"\\nTACTGGACTCGCAACGTGGGTCCATCAGTTGTCCGTATACCAAGACGTCTAAGGGCGGTGTACACCCTTTTGAGCAATGATTGCACAACCTGCGATCACCTTATACAGAATTATCAATCAAGCTCCCCGAGGAGCGGACTTGTAAGGACCGCCGCTTTCGCTCGGGTCTGCGG\\n+\\nFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF";}}' | gzip -c > {output.r1_extra}

seq 1 100 | awk ' {{print "@extra"$0"\\nCTTGTACTAGTGATGTTCTCCAGACAGGCTACAGATTTGATGGTTTTTTTTTTTTTTTTT\\n+\\nFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF";}}' | gzip -c > {output.r2_extra}

cat {input.part_r1} {output.r1_extra} > {output.r1}
cat {input.part_r2} {output.r2_extra} > {output.r2}
      """

## r2 is the cb + umi        
rule simulate_fastqs_from_a_cell:
    output:
        part_r1 =  temp(op.join(config['working_dir'], 'data', '{sample}',
                                'part_{cb1}_{cb2}_{cb3}_{umi}_r1.fq.gz')),
        part_r2 =  temp(op.join(config['working_dir'], 'data', '{sample}',
                                'part_{cb1}_{cb2}_{cb3}_{umi}_r2.fq.gz'))
    params:
        path = op.join(config['working_dir'], 'data', '{sample}'),
        tso_fix1 = "AATG",
        tso_fix2 = "CCAC",
        wta_fix1 = "GTGA",
        wta_fix2 = "GACA",
    threads:
        config['nthreads']
    shell:
        """
mkdir -p {params.path}

seq 1 10 | awk  '{{print "@wta"$0"_{wildcards.cb1}_{wildcards.cb2}_{wildcards.cb3}__{wildcards.umi}\\nTACTGGACTCGCAACGTGGGTCCATCAGTTGTCCGTATACCAAGACGTCTAAGGGCGGTGTACACCCTTTTGAGCAATGATTGCACAACCTGCGATCACCTTATACAGAATTATCAATCAAGCTCCCCGAGGAGCGGACTTGTAAGGACCGCCGCTTTCGCTCGGGTCTGCGG\\n+\\nFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF";}}' | gzip -c > {output.part_r1}

seq 1 10 | awk ' {{print "@wta"$0"_{wildcards.cb1}_{wildcards.cb2}_{wildcards.cb3}__{wildcards.umi}\\nA{wildcards.cb1}{params.wta_fix1}{wildcards.cb2}{params.wta_fix2}{wildcards.cb3}{wildcards.umi}AGATTTGATGGTTTTT\\n+\\nFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF";}}' | gzip -c > {output.part_r2}

## similarly for region 1000-1100 for ontargets/tso

seq 11 20 | awk ' {{print "@tso"$0"_{wildcards.cb1}_{wildcards.cb2}_{wildcards.cb3}__{wildcards.umi}\\nATTATTGAAGAGTTAGAACACCACAAGATAAAAAAAAAAAAAAAAAAAAAAAACAGCAGCGATTAAGGCAGAGGCGTTTGTATCTGCCATTATAAAGAAGTTTCCTCCAGCAACTCCTTTCTTAATTCCAAACTTAGC\\n+\\nFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF";}}' | gzip -c >> {output.part_r1}

seq 11 20 | awk  ' {{print "@tso"$0"_{wildcards.cb1}_{wildcards.cb2}_{wildcards.cb3}__{wildcards.umi}\\nT{wildcards.cb1}{params.tso_fix1}{wildcards.cb2}{params.tso_fix2}{wildcards.cb3}{wildcards.umi}TTTTTTTTTTTTTTTT\\n+\\nFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF";}}' | gzip -c >> {output.part_r2}

## another ontarget region - TSO barcode
## same UMI 100 times
seq 21 30 | awk ' {{print "@tso"$0"_{wildcards.cb1}_{wildcards.cb2}_{wildcards.cb3}__{wildcards.umi}\\nATTCCAAACTTAGCTTCAGTTATAAATTCCCCTCCCATGATTGGGATTTTATAAACTTTTCTTCCATATAATTCATCTTTCTTCTCATAACCGTCTCCGAAAAACTTCAACTTAAATCCAACCTTTAACTGCTCATCA\\n+\\nFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF";}}' | gzip -c >> {output.part_r1}

seq 21 30 | awk ' {{print "@tso"$0"_{wildcards.cb1}_{wildcards.cb2}_{wildcards.cb3}__{wildcards.umi}\\nG{wildcards.cb1}{params.tso_fix1}{wildcards.cb2}{params.tso_fix2}{wildcards.cb3}{wildcards.umi}TTTTTTTTTTTTTTTT\\n+\\nFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF";}}' | gzip -c >> {output.part_r2}
      """

# rule retrieve_whitelist:
#     output:
#         op.join(config['working_dir'], 'data', 'whitelists', 'BD_CLS1.txt'),
#         op.join(config['working_dir'], 'data', 'whitelists', 'BD_CLS2.txt'),
#         op.join(config['working_dir'], 'data', 'whitelists', 'BD_CLS3.txt')
#     params:
#         path = op.join(config['working_dir'], 'data', 'whitelists')
#     shell:
#         """
#         mkdir -p {params.path}
#         cd {params.path}
#         wget https://teichlab.github.io/scg_lib_structs/data/BD_CLS1.txt
#         wget https://teichlab.github.io/scg_lib_structs/data/BD_CLS2.txt
#         wget https://teichlab.github.io/scg_lib_structs/data/BD_CLS3.txt
#         """

## bd offers a couple of sets of whitelists, so we fetch the right one according to the config.yaml file
def symlink_whitelist(sample):
    os.makedirs(op.join(config['working_dir'], 'align_wta'), exist_ok = True)
                
    if get_barcode_whitelist_by_name(name = sample) == '96x3':
        for x in ['BD_CLS1.txt', 'BD_CLS2.txt', 'BD_CLS3.txt']:
            try:
                os.symlink(src = op.join(os.getcwd(), '..', 'data', 'whitelist_96x3', x),
                           dst = op.join(config['working_dir'], 'align_wta', sample, 'whitelists', x))
            except FileExistsError:
                break
    elif get_barcode_whitelist_by_name(name = sample) == '384x3':
        for x in ['BD_CLS1.txt', 'BD_CLS2.txt', 'BD_CLS3.txt']:
            try:
                os.symlink(src = op.join(os.getcwd(), '..', 'data', 'whitelist_384x3', x),
                           dst = op.join(config['working_dir'], 'align_wta', sample, 'whitelists', x))
            except FileExistsError:
                break

rule prepare_whitelists:
    input:
        cbumi = lambda wildcards: get_cbumi_by_name(wildcards.sample),
        cdna = lambda wildcards: get_cdna_by_name(wildcards.sample)
        # r1 = op.join(config['working_dir'], 'data', "{sample}", 'r1.fq.gz'),
        # r2 = op.join(config['working_dir'], 'data', "{sample}", 'r2.fq.gz')
    output:
        cb1 = op.join(config['working_dir'], 'align_wta', "{sample}", 'whitelists', 'BD_CLS1.txt'),
        cb2 = op.join(config['working_dir'], 'align_wta', "{sample}", 'whitelists', 'BD_CLS2.txt'),
        cb3 = op.join(config['working_dir'], 'align_wta', "{sample}", 'whitelists', 'BD_CLS3.txt')
    run:
        sample = wildcards.sample
        symlink_whitelist(sample)

# def create_run_mode_flag_files():
#     if config['run_mode'] == 'all':
#         with open(op.join(config['working_dir'], 'report_tso.flag'), 'w') as fh:
#             fh.write("flag")
#         with open(op.join(config['working_dir'], 'report_nidhis.flag'), 'w') as fh:
#             fh.write("flag") 
#     elif config['run_mode'] == 'tso off- and ontarget unique':
#         with open(op.join(config['working_dir'], 'report_tso.flag'), 'w') as fh:
#             fh.write("flag")
#     elif config['run_mode'] == 'tso ontarget multi':
#         with open(op.join(config['working_dir'], 'report_nidhis.flag'), 'w') as fh:
#             fh.write("flag")
#     with open(op.join(config['working_dir'], 'run_mode_done.flag'), 'w') as fh:
#             fh.write("flag")

    
# rule define_run_mode:
#     output:
#         cb1 = op.join(config['working_dir'], 'run_mode_done.flag'),
#     run:
#         create_run_mode_flag_files()

## caution we're hardcoding threads so one job is run at a time
rule align_wta:
    input:
        # r1 = op.join(config['working_dir'], 'data', "{sample}", 'r1.fq.gz'),
        # r2 = op.join(config['working_dir'], 'data', "{sample}", 'r2.fq.gz'),
        cdna = lambda wildcards: get_cdna_by_name(wildcards.sample),
        cbumi = lambda wildcards: get_cbumi_by_name(wildcards.sample),
        index_flag = op.join(config['working_dir'] , 'data', 'index', 'SAindex'),
        gtf = config['gtf'],
        cb1 = op.join(config['working_dir'], 'align_wta', "{sample}",  'whitelists', 'BD_CLS1.txt'),
        cb2 = op.join(config['working_dir'], 'align_wta', "{sample}", 'whitelists', 'BD_CLS2.txt'),
        cb3 = op.join(config['working_dir'], 'align_wta', "{sample}", 'whitelists', 'BD_CLS3.txt')
    output:
        bam = temp(op.join(config['working_dir'], 'align_wta', '{sample}', 'Aligned.sortedByCoord.out.bam')),
        filtered_barcodes = op.join(config['working_dir'], 'align_wta', '{sample}', 'Solo.out', 'Gene',
                                    'filtered', 'barcodes.tsv')
    threads: config['nthreads']
    params:
        path = op.join(config['working_dir'], 'align_wta', "{sample}/"),
        index_path = op.join(config['working_dir'] , 'data', 'index'),
        STAR = config['STAR'],
        # num_cells = get_expected_cells_by_name("{sample}"),
        num_cells = 2,
        tmp = op.join(config['working_dir'], 'tmp_align_wta_{sample}'),
        maxmem = config['per_task_max_mem'],
        sjdbOverhang = config['sjdbOverhang']
    shell:
        """
   rm -rf {params.tmp}
   mkdir -p {params.path} 

   {params.STAR} --runThreadN 5 \
     --genomeDir {params.index_path} \
     --readFilesCommand zcat \
     --outFileNamePrefix {params.path} \
     --readFilesIn  {input.cdna} {input.cbumi}  \
     --soloType CB_UMI_Complex \
     --soloAdapterSequence GTGANNNNNNNNNGACA \
     --soloCBposition 2_-9_2_-1 2_4_2_12 2_17_2_25 \
     --soloUMIposition 3_10_3_17 \
     --soloCBwhitelist {input.cb1} {input.cb2} {input.cb3} \
     --soloCBmatchWLtype 1MM \
     --soloCellFilter EmptyDrops_CR \
     --outSAMattributes NH HI AS nM NM MD jM jI MC ch CB UB gx gn sS CR CY UR UY \
     --soloCellReadStats Standard \
     --outSAMtype BAM SortedByCoordinate \
     --quantMode GeneCounts \
     --soloUMIlen 8 \
     --sjdbGTFfile {input.gtf} \
     --outTmpDir {params.tmp} \
     --sjdbOverhang {params.sjdbOverhang} \
     --limitBAMsortRAM {params.maxmem}

    rm -rf {params.tmp}
        """

        
# rule generate_tso_whitelist_from_wta_filtered:
#     input:
#         filtered_barcodes = op.join(config['working_dir'], 'align_wta', '{sample}', 'Solo.out', 'Gene',
#                                     'filtered', 'barcodes.tsv')
#     output:
#         tso_whitelist = op.join(config['working_dir'], 'data', '{sample}', 'tso_whitelist.txt')
#     params:
#         path = op.join(config['working_dir'], 'data', '{sample}')
#     shell:
#         """
#         mkdir -p {params.path}
#         awk -F "_" '{{print $1"AATG"$2"CCAC"$3}}' {input.filtered_barcodes} > {output.tso_whitelist}
#         """
        
## caution we're hardcoding threads so one job is run at a time
rule align_tso:
    input:
        # r1 = op.join(config['working_dir'], 'data', "{sample}", 'r1.fq.gz'),
        # r2 = op.join(config['working_dir'], 'data', "{sample}", 'r2.fq.gz'),
        cdna = lambda wildcards: get_cdna_by_name(wildcards.sample),
        cbumi = lambda wildcards: get_cbumi_by_name(wildcards.sample),
        index_flag = op.join(config['working_dir'] , 'data', 'index', 'SAindex'),
        gtf = config['gtf'],
        # tso_whitelist = op.join(config['working_dir'], 'data', '{sample}', 'tso_whitelist.txt'),
        # filtered_barcodes = op.join(config['working_dir'], 'align_wta', '{sample}', 'Solo.out', 'Gene',
        #                             'filtered', 'barcodes.tsv'),
        cb1 = op.join(config['working_dir'], 'align_wta', "{sample}",  'whitelists', 'BD_CLS1.txt'),
        cb2 = op.join(config['working_dir'], 'align_wta', "{sample}", 'whitelists', 'BD_CLS2.txt'),
        cb3 = op.join(config['working_dir'], 'align_wta', "{sample}", 'whitelists', 'BD_CLS3.txt')
    output:
        #bam = temp(op.join(config['working_dir'], 'align_tso', '{sample}', 'Aligned.sortedByCoord.out.bam'))
        bam = op.join(config['working_dir'], 'align_tso', '{sample}', 'Aligned.sortedByCoord.out.bam')
    threads: config['nthreads']
    params:
        path = op.join(config['working_dir'], 'align_tso', "{sample}/"),
        index_path = op.join(config['working_dir'] , 'data', 'index'),
        STAR = config['STAR'],
        tmp = op.join(config['working_dir'], 'tmp_align_tso_{sample}'),
        maxmem = config['per_task_max_mem'],
        sjdbOverhang = config['sjdbOverhang']
    shell:
        """
   rm -rf {params.tmp}
   mkdir -p {params.path} 

        ## with this whitelist the BAM looks good in CR terms but the count tables are broken (!)
        ## not reporting the issue 
        # #--soloCBwhitelist input.tso_whitelist
        #      # --soloCBmatchWLtype Exact 

        {params.STAR} --runThreadN 5 \
        --genomeDir {params.index_path} \
        --readFilesCommand zcat \
        --outFileNamePrefix {params.path} \
        --readFilesIn  {input.cdna} {input.cbumi}  \
        --soloType CB_UMI_Complex \
         --soloAdapterSequence AATGNNNNNNNNNCCAC \
         --soloCBposition 2_-9_2_-1 2_4_2_12 2_17_2_25 \
         --soloUMIposition 3_10_3_17 \
         --soloUMIlen 8 \
         --soloCellReadStats Standard \
         --soloCBwhitelist {input.cb1} {input.cb2} {input.cb3} \
         --soloCBmatchWLtype 1MM \
         --soloCellFilter None \
         --outSAMattributes NH HI AS nM NM MD jM jI MC ch CB UB gx gn sS CR CY UR UY\
         --outSAMtype BAM SortedByCoordinate \
         --quantMode GeneCounts \
        --sjdbGTFfile {input.gtf} \
        --outTmpDir {params.tmp} \
        --sjdbOverhang {params.sjdbOverhang} \
        --limitBAMsortRAM {params.maxmem}

        rm -rf {params.tmp}
        """

## this is a dirty workaround to reduce mem usage by selecting which chromosomes have 'captured' features
## @todo dedup and use wildcard for modality
rule subset_chromosomes_for_custom_counting:
    input:
        tso_bam = op.join(config['working_dir'], 'align_tso', '{sample}', 'Aligned.sortedByCoord.out.bam'),
        wta_bam = op.join(config['working_dir'], 'align_wta', '{sample}', 'Aligned.sortedByCoord.out.bam')
    output:
        red_tso_bam = op.join(config['working_dir'], 'align_tso', '{sample}', 'subset_tso.bam'),
        red_wta_bam = op.join(config['working_dir'], 'align_wta', '{sample}', 'subset_wta.bam'),
        # temp_red_tso_bam = temp(op.join(config['working_dir'], 'align_tso', '{sample}', 'temp_subset_tso.bam')),
        # temp_red_wta_bam = temp(op.join(config['working_dir'], 'align_wta', '{sample}', 'temp_subset_wta.bam')),
        chrs = temp(op.join(config['working_dir'], 'align_wta', '{sample}', 'captured_chrs.txt'))        
    threads: 10        
    params:
        gtf = config['gtf'],
        run_mode = config['run_mode'],
        pattern = config['capture_gtf_column_2_pattern']
    run:
        if params.run_mode in ['all', 'tso ontarget multi']:
            shell ("""
            grep {params.pattern} {params.gtf} | cut -f1 | sort | uniq  > {output.chrs}
            captured=$(cat {output.chrs} | tr '\n' ' ')

            samtools index -@ {threads} {input.tso_bam}
            samtools index -@ {threads} {input.wta_bam}

            samtools view -h {input.tso_bam}  $captured -@ {threads} | \
                samtools view -Sb > {output.red_tso_bam}
            samtools view -h {input.wta_bam} $captured -@ {threads} | \
                samtools view -Sb > {output.red_wta_bam}

            # ## plus rehead
            # samtools view -H [output.red_tso_bam] | \
            #   grep  -e "HD\|PG\|CO"  -f [output.chrs] | \
            #   samtools reheader --no-PG - [output.red_tso_bam] > [output.temp_red_tso_bam]

            # samtools view -H [output.red_wta_bam] | \
            #   grep  -e "HD\|PG\|CO"  -f [output.chrs] | \
            #   samtools reheader --no-PG - [output.red_wta_bam] > [output.temp_red_wta_bam]

            # cp [output.temp_red_tso_bam]  [output.red_tso_bam]
            # cp [output.temp_red_wta_bam]  [output.red_wta_bam]

            # samtools index -@ [threads] [output.red_tso_bam]
            # samtools index -@ [threads] [output.red_wta_bam]
            """)


rule subset_gtf_for_custom_counting:
    input:
        gtf = config['gtf']
    output:
        subset_gtf = temp(op.join(config['working_dir'], 'multimodal', 'subset.gtf'))
    threads: 10
    params:
        pattern = config['capture_gtf_column_2_pattern']
    shell:
        """
        grep {params.pattern} {input.gtf} > {output.subset_gtf}
        """
        
        
rule create_config_file_for_custom_counting:
    input:
        tso_bam = op.join(config['working_dir'], 'align_tso', '{sample}', 'subset_tso.bam'),
        wta_bam = op.join(config['working_dir'], 'align_wta', '{sample}', 'subset_wta.bam'),
        gtf = op.join(config['working_dir'], 'multimodal', 'subset.gtf')
    output:
        config = op.join(config['working_dir'], 'multimodal', '{sample}', 'config.yaml')
    threads: 10        
    params:
        tso_bam = op.join(config['working_dir'], 'align_tso', '{sample}', 'subset_tso.bam'),
        wta_bam = op.join(config['working_dir'], 'align_wta', '{sample}', 'subset_wta.bam'),
        gtf = op.join(config['working_dir'], 'multimodal', 'subset.gtf'),
        fc = 'featureCounts',
        parsing_threads = 5,
        fc_threads = 2,
        path = op.join(config['working_dir'], 'multimodal', '{sample}/'), ## mind the last slash
        output_bam_fn = '{sample}_tso_and_wta.bam',
        seqlogo = op.join(config['working_dir'], 'multimodal', '{sample}', 'seqlogo.png'),
        capture_pattern = config['capture_gtf_column_2_pattern'],
        run_mode = config['run_mode']
    run:
        if params.run_mode in ['all', 'tso ontarget multi']:
            shell ("""
mkdir -p {params.path}
cat <<EOF > {output.config}
# input file paths
input_bam_file_paths: 
  - {params.wta_bam}
  - {params.tso_bam}
unique_cb_bamfile_list:               # for valid cell bar codes extraction
  - {params.wta_bam}
gtf_file: {params.gtf}
featureCounts_path: {params.fc}

# input parameters
linker_pattern_for_seq_logo: ^([A-Z]{{9}})(AATG[A-Z]{{9}}CCAC) # tso pattern to get tso tail seq logo
linker_pattern_list:                              # list of linkers to be matched with alignment. If no match then CB tag is recovered where possible.
  - ^([A-Z]{{9}})(AATG[A-Z]{{9}}CCAC)             # tso linkers
  - ^(A|GT|TCA)?([A-Z]{{9}})(GTGA[A-Z]{{9}}GACA)  # wta linkers
seq_logo_filename: {params.seqlogo}
cb1_len: 9                            # length of first part of CB tag
cb2_len: 9                            # length of second part of CB tag
cb3_len: 9                            # length of third part of CB tag
linker_left_len: 4                    # length of linker on left
linker_right_len: 4                   # length of linker on right
umi_len: 8                            # length of UMI in sS tag
chunk_genomic_area: 1000              # number of base pairs to be processed by each process in multiprocessing

# NOTE: number of rg tag names must be equal to the number of input bam files and in the order of input bam files
rg_tag_names:
  - wta
  - tso 

chromosomes:                            # chromosomes to be processed (None means all of them)
  - egfp
  - chr5

subset_gtf: 1                           # subset gtf file by pattern (1: yes, 0: no)
subset_gtf_output_file: ontarget.gtf    # output file name of subset of gtf file
subset_gtf_pattern: {params.capture_pattern}  # filter gtf file by this pattern if subset_gtf = 1
write_final_bam_to_csv: 0               # write final merged bam file to a csv file(1: yes, 0: no)
write_final_bam_header_to_txt: 0        # write final merged bam header to text file (1: yes, 0: no)
nProcessors: {params.parsing_threads}   # for multiprocessing
nthreads: {params.fc_threads}           # for featureCounts

# output file names and paths
output_folder: {params.path}
final_merged_file: {params.output_bam_fn}
featureCounts_output_file: featurecounted
log_file: log.txt
error_log: error_log.txt
EOF
        """)
        
rule count_custom_regions_tso:
    input:
        tso_bam = op.join(config['working_dir'], 'align_tso', '{sample}', 'subset_tso.bam'),
        wta_bam = op.join(config['working_dir'], 'align_wta', '{sample}', 'subset_wta.bam'),
        config = op.join(config['working_dir'], 'multimodal', '{sample}', 'config.yaml'),
        #gtf = config['gtf'],
        gtf = op.join(config['working_dir'], 'multimodal', 'subset.gtf')
    output:
        merged_bam = op.join(config['working_dir'], 'multimodal', '{sample}', '{sample}_tso_and_wta.bam'),
        fc = op.join(config['working_dir'], 'multimodal', '{sample}', 'featurecounted')
    threads: 10
    resources:
         mem_mb=100000
    params:
        module_path = op.join('module', 'src', 'main'),
        run_mode = config['run_mode']
    run:
        if params.run_mode in ['all', 'tso ontarget multi']:
            shell ("""
            # cd {params.module_path}
            ln -s -f {params.module_path} .
            ln -s -f {params.module_path}/../utilities .
            python -m  main.combined_rock_roi --config {input.config} &> /dev/null
            """)

rule generate_sce:
    input:
        tso_bam = op.join(config['working_dir'], 'align_tso', '{sample}', 'Aligned.sortedByCoord.out.bam'),
        wta_bam = op.join(config['working_dir'], 'align_wta', '{sample}', 'Aligned.sortedByCoord.out.bam'),
        fc = op.join(config['working_dir'], 'multimodal', '{sample}', 'featurecounted'),
        # config = op.join(config['working_dir'], 'multimodal', '{sample}', 'config.yaml'),
        gtf = config['gtf']
    output:
        sce = op.join(config['working_dir'], 'multimodal', '{sample}', '{sample}_sce.rds')
    params:
        run_mode = config['run_mode'],
        working_dir = config['working_dir'],
        sample = "{wildcards.sample}"
    # run:
    #     if params.run_mode in ['all', 'tso ontarget multi']:
    #         shell ("""
    #         echo 'all or multi' > {output.sce}
    #         """)
    #     else:
    #         shell(" echo 'something else' > {output.sce}")
    shell:
        """
        touch {output.sce}
        # Rscript generate_sce_object.R --sample {wildcards.sample} \
        #      --run_mode {params.run_mode} \
        #      --working_dir {params.working_dir} \
        #      --output_fn {output.sce}
        """
